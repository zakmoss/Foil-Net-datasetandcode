{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "tW_BPubIngrs"
   },
   "outputs": [],
   "source": [
    "#importing all the packages\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import functional as F\n",
    "from skimage import io, transform\n",
    "from torch.optim import lr_scheduler\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from tqdm import tqdm\n",
    "device = torch.device('cuda')\n",
    "#import albumentations as A\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "nKEgxw52nkKZ"
   },
   "outputs": [],
   "source": [
    "#create dataset class to return each batch\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class SurfAutoencoder(Dataset):\n",
    "    def __init__(self, csv_file, augment=False):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.augment = augment\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "      #  if self.augment:\n",
    "          #  self.augmentation_transform = A.Compose([\n",
    "            #    A.Rotate(limit = 90, p=1)\n",
    "           # ])\n",
    "      #  else:\n",
    "           # self.augmentation_transform = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        first_image = cv2.imread(self.annotations.iloc[index, 0])\n",
    "       # if self.augmentation_transform is not None:\n",
    "           # augmented = self.augmentation_transform(image=first_image)\n",
    "           # first_image = augmented[\"image\"]\n",
    "\n",
    "        first_image = self.transform(first_image)\n",
    "\n",
    "        return first_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_jX7iYvn0Al",
    "outputId": "fc062aba-960d-460e-8b9d-cd8613cafaca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18702\n",
      "tensor([[[0.9882, 0.9843, 0.9882,  ..., 0.2431, 0.2157, 0.2000],\n",
      "         [0.9804, 0.9804, 0.9804,  ..., 0.1333, 0.2157, 0.2353],\n",
      "         [0.9804, 0.9765, 0.9804,  ..., 0.0824, 0.1412, 0.1255],\n",
      "         ...,\n",
      "         [0.4784, 0.4824, 0.4941,  ..., 0.3922, 0.3176, 0.3373],\n",
      "         [0.4784, 0.4824, 0.4863,  ..., 0.4863, 0.4118, 0.3412],\n",
      "         [0.4784, 0.4863, 0.4706,  ..., 0.2588, 0.3843, 0.3569]],\n",
      "\n",
      "        [[0.9961, 1.0000, 1.0000,  ..., 0.3098, 0.2784, 0.2588],\n",
      "         [0.9922, 0.9961, 1.0000,  ..., 0.1843, 0.2627, 0.2824],\n",
      "         [0.9882, 0.9882, 0.9922,  ..., 0.1216, 0.1725, 0.1569],\n",
      "         ...,\n",
      "         [0.4824, 0.4784, 0.4980,  ..., 0.4196, 0.3333, 0.3490],\n",
      "         [0.4784, 0.4784, 0.4902,  ..., 0.5098, 0.4275, 0.3569],\n",
      "         [0.4784, 0.4863, 0.4745,  ..., 0.2824, 0.3961, 0.3686]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000,  ..., 0.3137, 0.2824, 0.2549],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.1882, 0.2627, 0.2784],\n",
      "         [0.9961, 0.9961, 0.9961,  ..., 0.1216, 0.1725, 0.1490],\n",
      "         ...,\n",
      "         [0.3373, 0.3294, 0.3333,  ..., 0.1176, 0.0275, 0.0275],\n",
      "         [0.3373, 0.3255, 0.3216,  ..., 0.2275, 0.1608, 0.0824],\n",
      "         [0.3373, 0.3255, 0.3059,  ..., 0.0039, 0.1216, 0.1020]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "dataset = SurfAutoencoder(csv_file=\"C:\\\\Users\\\\Pepperdine\\\\Downloads\\\\output1.csv\", augment=True)\n",
    "\n",
    "print(dataset.__len__())\n",
    "torch.manual_seed(seed)\n",
    "train_size = 16917\n",
    "test_size = 1785\n",
    "total_size = train_size + test_size\n",
    "\n",
    "train_set, test_set = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(train_set.__getitem__(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=1024, z_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar\n",
    "import torch\n",
    "\n",
    "dummy_tensor = torch.randn(1, 3, 64, 64)\n",
    "model = VAE()\n",
    "output, mu, logvar = model(dummy_tensor)\n",
    "\n",
    "print(output.shape)\n",
    "#this code is working....l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aT3_Ap4CoRZY",
    "outputId": "b7174041-3c0f-4ff6-ef86-3bd6f0822b58"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APrHnmhxoPLh",
    "outputId": "c18360c8-027a-486d-f589-365169c37afb",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "1x3FUBJXJFFM"
   },
   "outputs": [],
   "source": [
    "#it working well dont stop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "P1jCaY9u9F9z",
    "outputId": "3753e28b-8541-493e-873e-fcb12ad6369d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.9961, 0.9961, 1.0000,  ..., 0.7216, 0.7137, 0.7137],\n",
      "          [0.9961, 0.9922, 0.9961,  ..., 0.7333, 0.7412, 0.7412],\n",
      "          [1.0000, 0.9961, 0.9922,  ..., 0.7255, 0.7216, 0.7098],\n",
      "          ...,\n",
      "          [0.4196, 0.4196, 0.4039,  ..., 0.3294, 0.3255, 0.3216],\n",
      "          [0.4118, 0.3922, 0.3647,  ..., 0.3216, 0.3176, 0.3216],\n",
      "          [0.3922, 0.3686, 0.3647,  ..., 0.3137, 0.3098, 0.3255]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 0.6353, 0.6314, 0.6392],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.6706, 0.6745, 0.6667],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.6627, 0.6471, 0.6314],\n",
      "          ...,\n",
      "          [0.3725, 0.3725, 0.3451,  ..., 0.2588, 0.2471, 0.2431],\n",
      "          [0.3686, 0.3451, 0.3098,  ..., 0.2627, 0.2510, 0.2549],\n",
      "          [0.3490, 0.3098, 0.3059,  ..., 0.2627, 0.2549, 0.2706]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 0.5294, 0.5333, 0.5451],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.5922, 0.5922, 0.5804],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.5882, 0.5608, 0.5373],\n",
      "          ...,\n",
      "          [0.1882, 0.1882, 0.1569,  ..., 0.0039, 0.0000, 0.0000],\n",
      "          [0.1843, 0.1569, 0.1098,  ..., 0.0196, 0.0078, 0.0039],\n",
      "          [0.1490, 0.1098, 0.0980,  ..., 0.0353, 0.0118, 0.0235]]],\n",
      "\n",
      "\n",
      "        [[[0.1961, 0.2039, 0.2275,  ..., 0.6196, 0.6314, 0.6196],\n",
      "          [0.1961, 0.2039, 0.2157,  ..., 0.6235, 0.6275, 0.6196],\n",
      "          [0.1843, 0.1961, 0.2118,  ..., 0.6235, 0.6196, 0.6039],\n",
      "          ...,\n",
      "          [0.5059, 0.5098, 0.5176,  ..., 0.9451, 0.9451, 0.9451],\n",
      "          [0.5020, 0.5059, 0.5098,  ..., 0.9529, 0.9451, 0.9451],\n",
      "          [0.4980, 0.4941, 0.4863,  ..., 0.9569, 0.9529, 0.9490]],\n",
      "\n",
      "         [[0.1569, 0.1686, 0.1882,  ..., 0.5961, 0.6000, 0.6000],\n",
      "          [0.1529, 0.1647, 0.1882,  ..., 0.6000, 0.6000, 0.6000],\n",
      "          [0.1490, 0.1608, 0.1804,  ..., 0.6000, 0.5922, 0.5922],\n",
      "          ...,\n",
      "          [0.3765, 0.3765, 0.3765,  ..., 0.9451, 0.9451, 0.9451],\n",
      "          [0.3725, 0.3725, 0.3725,  ..., 0.9529, 0.9451, 0.9451],\n",
      "          [0.3765, 0.3647, 0.3490,  ..., 0.9569, 0.9529, 0.9490]],\n",
      "\n",
      "         [[0.0039, 0.0235, 0.0392,  ..., 0.5294, 0.5373, 0.5412],\n",
      "          [0.0000, 0.0118, 0.0353,  ..., 0.5373, 0.5412, 0.5451],\n",
      "          [0.0000, 0.0039, 0.0275,  ..., 0.5373, 0.5412, 0.5451],\n",
      "          ...,\n",
      "          [0.2157, 0.2078, 0.2118,  ..., 0.9451, 0.9451, 0.9451],\n",
      "          [0.2118, 0.2078, 0.2078,  ..., 0.9490, 0.9490, 0.9451],\n",
      "          [0.2157, 0.2000, 0.1843,  ..., 0.9529, 0.9529, 0.9490]]],\n",
      "\n",
      "\n",
      "        [[[0.5922, 0.5882, 0.5843,  ..., 0.7373, 0.7373, 0.7412],\n",
      "          [0.5882, 0.5843, 0.5804,  ..., 0.7412, 0.7373, 0.7294],\n",
      "          [0.5765, 0.5765, 0.5765,  ..., 0.7255, 0.7176, 0.7176],\n",
      "          ...,\n",
      "          [0.1569, 0.1569, 0.1569,  ..., 0.3333, 0.3059, 0.2627],\n",
      "          [0.1569, 0.1569, 0.1529,  ..., 0.3569, 0.3255, 0.2980],\n",
      "          [0.1529, 0.1529, 0.1490,  ..., 0.3490, 0.3373, 0.3294]],\n",
      "\n",
      "         [[0.5451, 0.5412, 0.5373,  ..., 0.7725, 0.7882, 0.7922],\n",
      "          [0.5451, 0.5451, 0.5412,  ..., 0.7804, 0.7843, 0.7804],\n",
      "          [0.5412, 0.5412, 0.5412,  ..., 0.7647, 0.7647, 0.7686],\n",
      "          ...,\n",
      "          [0.1490, 0.1490, 0.1490,  ..., 0.3490, 0.3176, 0.2784],\n",
      "          [0.1451, 0.1451, 0.1451,  ..., 0.3765, 0.3412, 0.3137],\n",
      "          [0.1412, 0.1412, 0.1412,  ..., 0.3686, 0.3569, 0.3451]],\n",
      "\n",
      "         [[0.5176, 0.5137, 0.5098,  ..., 0.7725, 0.8000, 0.8039],\n",
      "          [0.5176, 0.5176, 0.5137,  ..., 0.7804, 0.7961, 0.7922],\n",
      "          [0.5137, 0.5137, 0.5137,  ..., 0.7647, 0.7765, 0.7804],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.2353, 0.2118, 0.1725],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.2824, 0.2471, 0.2118],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.2863, 0.2706, 0.2431]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.6627, 0.6667, 0.6745,  ..., 0.6353, 0.6314, 0.6235],\n",
      "          [0.6588, 0.6588, 0.6627,  ..., 0.6392, 0.6471, 0.6353],\n",
      "          [0.6706, 0.6706, 0.6627,  ..., 0.6471, 0.6431, 0.6196],\n",
      "          ...,\n",
      "          [0.2314, 0.2275, 0.2392,  ..., 0.3216, 0.3098, 0.3529],\n",
      "          [0.2196, 0.2392, 0.2627,  ..., 0.3216, 0.3294, 0.5529],\n",
      "          [0.2314, 0.2510, 0.2588,  ..., 0.2627, 0.3098, 0.5490]],\n",
      "\n",
      "         [[0.6431, 0.6471, 0.6549,  ..., 0.6353, 0.6392, 0.6431],\n",
      "          [0.6392, 0.6392, 0.6431,  ..., 0.6353, 0.6510, 0.6510],\n",
      "          [0.6510, 0.6510, 0.6431,  ..., 0.6353, 0.6392, 0.6275],\n",
      "          ...,\n",
      "          [0.1961, 0.1922, 0.2157,  ..., 0.2941, 0.2824, 0.3255],\n",
      "          [0.1765, 0.2039, 0.2314,  ..., 0.2941, 0.3020, 0.5255],\n",
      "          [0.1843, 0.2118, 0.2196,  ..., 0.2353, 0.2824, 0.5216]],\n",
      "\n",
      "         [[0.6039, 0.6078, 0.6157,  ..., 0.5804, 0.5725, 0.5686],\n",
      "          [0.6000, 0.6000, 0.6039,  ..., 0.5765, 0.5804, 0.5765],\n",
      "          [0.6118, 0.6118, 0.6039,  ..., 0.5804, 0.5725, 0.5569],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.1333, 0.1294, 0.1725],\n",
      "          [0.0000, 0.0000, 0.0118,  ..., 0.1333, 0.1451, 0.3686],\n",
      "          [0.0000, 0.0039, 0.0118,  ..., 0.0745, 0.1216, 0.3608]]],\n",
      "\n",
      "\n",
      "        [[[0.9412, 0.9412, 0.9412,  ..., 0.7373, 0.7529, 0.7725],\n",
      "          [0.9373, 0.9373, 0.9373,  ..., 0.6863, 0.6902, 0.7137],\n",
      "          [0.9294, 0.9294, 0.9294,  ..., 0.6706, 0.7098, 0.6745],\n",
      "          ...,\n",
      "          [0.3176, 0.3216, 0.3216,  ..., 0.3098, 0.3333, 0.3451],\n",
      "          [0.3216, 0.3216, 0.3216,  ..., 0.3608, 0.3176, 0.3451],\n",
      "          [0.3451, 0.3412, 0.3333,  ..., 0.3961, 0.3255, 0.3451]],\n",
      "\n",
      "         [[0.9412, 0.9412, 0.9412,  ..., 0.7176, 0.7529, 0.7686],\n",
      "          [0.9373, 0.9373, 0.9373,  ..., 0.6667, 0.6667, 0.6980],\n",
      "          [0.9294, 0.9294, 0.9294,  ..., 0.6353, 0.6706, 0.6353],\n",
      "          ...,\n",
      "          [0.3137, 0.3176, 0.3176,  ..., 0.2784, 0.3176, 0.3412],\n",
      "          [0.3176, 0.3176, 0.3176,  ..., 0.3333, 0.3059, 0.3412],\n",
      "          [0.3412, 0.3373, 0.3333,  ..., 0.3686, 0.3059, 0.3294]],\n",
      "\n",
      "         [[0.9412, 0.9412, 0.9412,  ..., 0.6118, 0.6471, 0.6745],\n",
      "          [0.9373, 0.9373, 0.9373,  ..., 0.5412, 0.5451, 0.5882],\n",
      "          [0.9294, 0.9294, 0.9294,  ..., 0.4941, 0.5333, 0.5020],\n",
      "          ...,\n",
      "          [0.1608, 0.1647, 0.1647,  ..., 0.0784, 0.1059, 0.1373],\n",
      "          [0.1647, 0.1647, 0.1647,  ..., 0.1333, 0.0980, 0.1255],\n",
      "          [0.1843, 0.1843, 0.1765,  ..., 0.1686, 0.1059, 0.1176]]],\n",
      "\n",
      "\n",
      "        [[[0.6745, 0.6902, 0.6941,  ..., 0.7255, 0.7059, 0.7216],\n",
      "          [0.6784, 0.6745, 0.6627,  ..., 0.7255, 0.7255, 0.7216],\n",
      "          [0.7608, 0.7647, 0.7647,  ..., 0.7137, 0.7098, 0.7137],\n",
      "          ...,\n",
      "          [0.2549, 0.2549, 0.2549,  ..., 0.3412, 0.2667, 0.2510],\n",
      "          [0.2549, 0.2549, 0.2549,  ..., 0.4902, 0.4000, 0.3176],\n",
      "          [0.2549, 0.2510, 0.2510,  ..., 0.4588, 0.4196, 0.4706]],\n",
      "\n",
      "         [[0.5098, 0.5098, 0.5176,  ..., 0.6941, 0.6784, 0.6941],\n",
      "          [0.5608, 0.5451, 0.5294,  ..., 0.6941, 0.6941, 0.6902],\n",
      "          [0.7020, 0.6941, 0.6784,  ..., 0.6824, 0.6824, 0.6824],\n",
      "          ...,\n",
      "          [0.2118, 0.2118, 0.2118,  ..., 0.3333, 0.2392, 0.2235],\n",
      "          [0.2118, 0.2118, 0.2078,  ..., 0.4784, 0.3804, 0.2980],\n",
      "          [0.2118, 0.2078, 0.2039,  ..., 0.4431, 0.4078, 0.4706]],\n",
      "\n",
      "         [[0.4588, 0.4549, 0.4549,  ..., 0.7490, 0.7294, 0.7412],\n",
      "          [0.5569, 0.5373, 0.5137,  ..., 0.7490, 0.7451, 0.7373],\n",
      "          [0.7412, 0.7333, 0.7176,  ..., 0.7333, 0.7294, 0.7294],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.1961, 0.0392, 0.0118],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.3725, 0.2157, 0.1059],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.3529, 0.2627, 0.2941]]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x36864 and 1024x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, images \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m---> 15\u001b[0m         recon_images, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m         loss, bce, kld \u001b[38;5;241m=\u001b[39m loss_fn(recon_images, images, mu, logvar)\n\u001b[0;32m     17\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[58], line 55\u001b[0m, in \u001b[0;36mVAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 55\u001b[0m     z, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(z)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z, mu, logvar\n",
      "Cell \u001b[1;32mIn[58], line 46\u001b[0m, in \u001b[0;36mVAE.encode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     45\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m---> 46\u001b[0m     z, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbottleneck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z, mu, logvar\n",
      "Cell \u001b[1;32mIn[58], line 40\u001b[0m, in \u001b[0;36mVAE.bottleneck\u001b[1;34m(self, h)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbottleneck\u001b[39m(\u001b[38;5;28mself\u001b[39m, h):\n\u001b[1;32m---> 40\u001b[0m     mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(h)\n\u001b[0;32m     41\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, logvar)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z, mu, logvar\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x36864 and 1024x32)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "epochs = 100\n",
    "vae = VAE()\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for idx, images in enumerate(train_loader):\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "        loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, epochs, loss.data[0]/bs, bce.data[0]/bs, kld.data[0]/bs)\n",
    "        print(to_print)\n",
    "\n",
    "    # notify to android when finished training\n",
    "    notify(to_print, priority=1)\n",
    "\n",
    "torch.save(vae.state_dict(), 'vae.torch')\n",
    "\n",
    "\n",
    "checkpoint_path = \"C:/Users/Pepperdine/Desktop/cp_more_complex/model_epoch_100.pt\"\n",
    "autoencoder = Autoencoder()\n",
    "autoencoder.load_state_dict(torch.load(checkpoint_path))\n",
    "autoencoder.to(device)\n",
    "autoencoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        encoded, reconstructions = autoencoder(images)\n",
    "        break\n",
    "\n",
    "images = images.cpu().numpy()\n",
    "reconstructions = reconstructions.cpu().numpy()\n",
    "\n",
    "if images.ndim == 4:\n",
    "    images = np.transpose(images, (0, 2, 3, 1))\n",
    "if reconstructions.ndim == 4:\n",
    "    reconstructions = np.transpose(reconstructions, (0, 2, 3, 1))\n",
    "num_images = min(5, len(images))\n",
    "fig, axes = plt.subplots(nrows=2, ncols=num_images, figsize=(20, 10))\n",
    "\n",
    "for i in range(num_images):\n",
    "    axes[0, i].imshow(images[i].squeeze(), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title(\"Original\")\n",
    "\n",
    "    axes[1, i].imshow(reconstructions[i].squeeze(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    axes[1, i].set_title(\"Reconstructed\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
